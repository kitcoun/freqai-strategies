from enum import IntEnum
import logging
import json
import time
import numpy as np
import pandas as pd
import scipy as sp
import optuna
import sklearn
import warnings
import talib.abstract as ta

from functools import cached_property
from typing import Any, Callable, Optional
from pathlib import Path
from freqtrade.freqai.base_models.BaseRegressionModel import BaseRegressionModel
from freqtrade.freqai.data_kitchen import FreqaiDataKitchen


TEST_SIZE = 0.1

EXTREMA_COLUMN = "&s-extrema"
MINIMA_THRESHOLD_COLUMN = "&s-minima_threshold"
MAXIMA_THRESHOLD_COLUMN = "&s-maxima_threshold"

warnings.simplefilter(action="ignore", category=FutureWarning)

logger = logging.getLogger(__name__)


class QuickAdapterRegressorV3(BaseRegressionModel):
    """
    The following freqaimodel is released to sponsors of the non-profit FreqAI open-source project.
    If you find the FreqAI project useful, please consider supporting it by becoming a sponsor.
    We use sponsor money to help stimulate new features and to pay for running these public
    experiments, with a an objective of helping the community make smarter choices in their
    ML journey.

    This freqaimodel is experimental (as with all models released to sponsors). Do *not* expect
    returns. The goal is to demonstrate gratitude to people who support the project and to
    help them find a good starting point for their own creativity.

    If you have questions, please direct them to our discord: https://discord.gg/xE4RMg4QYw

    https://github.com/sponsors/robcaulk
    """

    version = "3.7.72"

    @cached_property
    def _optuna_config(self) -> dict:
        optuna_default_config = {
            "enabled": False,
            "n_jobs": min(
                self.freqai_info.get("optuna_hyperopt", {}).get("n_jobs", 1),
                max(int(self.max_system_threads / 4), 1),
            ),
            "storage": "file",
            "continuous": True,
            "warm_start": True,
            "n_startup_trials": 15,
            "n_trials": 36,
            "timeout": 7200,
            "candles_step": 10,
            "seed": 1,
        }
        return {
            **optuna_default_config,
            **self.freqai_info.get("optuna_hyperopt", {}),
        }

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.pairs = self.config.get("exchange", {}).get("pair_whitelist")
        if not self.pairs:
            raise ValueError(
                "FreqAI model requires StaticPairList method defined in pairlists configuration and 'pair_whitelist' defined in exchange section configuration"
            )
        if (
            self.freqai_info.get("identifier") is None
            or self.freqai_info.get("identifier").strip() == ""
        ):
            raise ValueError(
                "FreqAI model requires 'identifier' defined in the freqai section configuration"
            )
        self._optuna_hyperopt: bool = (
            self.freqai_info.get("enabled", False)
            and self._optuna_config.get("enabled")
            and self.data_split_parameters.get("test_size", TEST_SIZE) > 0
        )
        self._optuna_hp_value: dict[str, float] = {}
        self._optuna_train_value: dict[str, float] = {}
        self._optuna_label_values: dict[str, list] = {}
        self._optuna_hp_params: dict[str, dict] = {}
        self._optuna_train_params: dict[str, dict] = {}
        self._optuna_label_params: dict[str, dict] = {}
        for pair in self.pairs:
            self._optuna_hp_value[pair] = -1
            self._optuna_train_value[pair] = -1
            self._optuna_label_values[pair] = [-1, -1]
            self._optuna_hp_params[pair] = (
                self.optuna_load_best_params(pair, "hp")
                if self.optuna_load_best_params(pair, "hp")
                else {}
            )
            self._optuna_train_params[pair] = (
                self.optuna_load_best_params(pair, "train")
                if self.optuna_load_best_params(pair, "train")
                else {}
            )
            self._optuna_label_params[pair] = (
                self.optuna_load_best_params(pair, "label")
                if self.optuna_load_best_params(pair, "label")
                else {
                    "label_period_candles": self.ft_params.get(
                        "label_period_candles", 50
                    ),
                    "label_natr_ratio": float(
                        self.ft_params.get("label_natr_ratio", 6.0)
                    ),
                }
            )
        logger.info(
            f"Initialized {self.__class__.__name__} {self.freqai_info.get('regressor', 'xgboost')} regressor model version {self.version}"
        )

    def get_optuna_params(self, pair: str, namespace: str) -> dict[str, Any]:
        if namespace == "hp":
            params = self._optuna_hp_params.get(pair)
        elif namespace == "train":
            params = self._optuna_train_params.get(pair)
        elif namespace == "label":
            params = self._optuna_label_params.get(pair)
        else:
            raise ValueError(f"Invalid namespace: {namespace}")
        return params

    def set_optuna_params(self, pair: str, namespace: str, params: dict) -> None:
        if namespace == "hp":
            self._optuna_hp_params[pair] = params
        elif namespace == "train":
            self._optuna_train_params[pair] = params
        elif namespace == "label":
            self._optuna_label_params[pair] = params
        else:
            raise ValueError(f"Invalid namespace: {namespace}")

    def get_optuna_value(self, pair: str, namespace: str) -> float:
        if namespace == "hp":
            value = self._optuna_hp_value.get(pair)
        elif namespace == "train":
            value = self._optuna_train_value.get(pair)
        else:
            raise ValueError(f"Invalid namespace: {namespace}")
        return value

    def set_optuna_value(self, pair: str, namespace: str, value: float) -> None:
        if namespace == "hp":
            self._optuna_hp_value[pair] = value
        elif namespace == "train":
            self._optuna_train_value[pair] = value
        else:
            raise ValueError(f"Invalid namespace: {namespace}")

    def get_optuna_values(self, pair: str, namespace: str) -> list:
        if namespace == "label":
            values = self._optuna_label_values.get(pair)
        else:
            raise ValueError(f"Invalid namespace: {namespace}")
        return values

    def set_optuna_values(self, pair: str, namespace: str, values: list) -> None:
        if namespace == "label":
            self._optuna_label_values[pair] = values
        else:
            raise ValueError(f"Invalid namespace: {namespace}")

    def fit(self, data_dictionary: dict, dk: FreqaiDataKitchen, **kwargs) -> Any:
        """
        User sets up the training and test data to fit their desired model here
        :param data_dictionary: the dictionary constructed by DataHandler to hold
                                all the training and test data/labels.
        :param dk: the FreqaiDataKitchen object
        """

        X = data_dictionary["train_features"]
        y = data_dictionary["train_labels"]
        train_weights = data_dictionary["train_weights"]

        X_test = data_dictionary["test_features"]
        y_test = data_dictionary["test_labels"]
        test_weights = data_dictionary["test_weights"]

        model_training_parameters = self.model_training_parameters

        start = time.time()
        if self._optuna_hyperopt:
            self.optuna_optimize(
                pair=dk.pair,
                namespace="label",
                objective=lambda trial: label_objective(
                    trial,
                    self.data_provider.get_pair_dataframe(dk.pair),
                    self.freqai_info.get("fit_live_predictions_candles", 100),
                    self._optuna_config.get("candles_step"),
                ),
                directions=[
                    optuna.study.StudyDirection.MAXIMIZE,
                    optuna.study.StudyDirection.MAXIMIZE,
                ],
            )

            self.optuna_optimize(
                pair=dk.pair,
                namespace="hp",
                objective=lambda trial: hp_objective(
                    trial,
                    self.freqai_info.get("regressor", "xgboost"),
                    X,
                    y,
                    train_weights,
                    X_test,
                    y_test,
                    test_weights,
                    model_training_parameters,
                ),
                direction=optuna.study.StudyDirection.MINIMIZE,
            )

            optuna_hp_params = self.get_optuna_params(dk.pair, "hp")
            if optuna_hp_params:
                model_training_parameters = {
                    **model_training_parameters,
                    **optuna_hp_params,
                }

            self.optuna_optimize(
                pair=dk.pair,
                namespace="train",
                objective=lambda trial: train_objective(
                    trial,
                    self.freqai_info.get("regressor", "xgboost"),
                    X,
                    y,
                    train_weights,
                    X_test,
                    y_test,
                    test_weights,
                    self.data_split_parameters.get("test_size", TEST_SIZE),
                    self.freqai_info.get("fit_live_predictions_candles", 100),
                    self._optuna_config.get("candles_step"),
                    model_training_parameters,
                ),
                direction=optuna.study.StudyDirection.MINIMIZE,
            )

            optuna_train_params = self.get_optuna_params(dk.pair, "train")
            if optuna_train_params:
                train_window = optuna_train_params.get("train_period_candles")
                X = X.iloc[-train_window:]
                y = y.iloc[-train_window:]
                train_weights = train_weights[-train_window:]

                test_window = optuna_train_params.get("test_period_candles")
                X_test = X_test.iloc[-test_window:]
                y_test = y_test.iloc[-test_window:]
                test_weights = test_weights[-test_window:]

        eval_set, eval_weights = self.eval_set_and_weights(X_test, y_test, test_weights)

        model = fit_regressor(
            regressor=self.freqai_info.get("regressor", "xgboost"),
            X=X,
            y=y,
            train_weights=train_weights,
            eval_set=eval_set,
            eval_weights=eval_weights,
            model_training_parameters=model_training_parameters,
            init_model=self.get_init_model(dk.pair),
        )
        time_spent = time.time() - start
        self.dd.update_metric_tracker("fit_time", time_spent, dk.pair)

        return model

    def fit_live_predictions(self, dk: FreqaiDataKitchen, pair: str) -> None:
        warmed_up = True

        fit_live_predictions_candles = self.freqai_info.get(
            "fit_live_predictions_candles", 100
        )

        if self.live:
            if not hasattr(self, "exchange_candles"):
                self.exchange_candles = len(self.dd.model_return_values[pair].index)
            candles_diff = len(self.dd.historic_predictions[pair].index) - (
                fit_live_predictions_candles + self.exchange_candles
            )
            if candles_diff < 0:
                logger.warning(
                    f"{pair}: fit live predictions not warmed up yet. Still {abs(candles_diff)} candles to go."
                )
                warmed_up = False

        pred_df_full = (
            self.dd.historic_predictions[pair]
            .iloc[-fit_live_predictions_candles:]
            .reset_index(drop=True)
        )

        if not warmed_up:
            dk.data["extra_returns_per_train"][MINIMA_THRESHOLD_COLUMN] = -2
            dk.data["extra_returns_per_train"][MAXIMA_THRESHOLD_COLUMN] = 2
        else:
            min_pred, max_pred = self.min_max_pred(
                pred_df_full,
                fit_live_predictions_candles,
                self.get_optuna_params(pair, "label").get("label_period_candles"),
            )
            dk.data["extra_returns_per_train"][MINIMA_THRESHOLD_COLUMN] = min_pred
            dk.data["extra_returns_per_train"][MAXIMA_THRESHOLD_COLUMN] = max_pred

        dk.data["labels_mean"], dk.data["labels_std"] = {}, {}
        for label in dk.label_list + dk.unique_class_list:
            if pred_df_full[label].dtype == object:
                continue
            if not warmed_up:
                f = [0, 0]
            else:
                f = sp.stats.norm.fit(pred_df_full[label])
            dk.data["labels_mean"][label], dk.data["labels_std"][label] = f[0], f[1]

        # fit the DI_threshold
        if not warmed_up:
            f = [0, 0, 0]
            cutoff = 2
        else:
            di_values = pd.to_numeric(pred_df_full["DI_values"], errors="coerce")
            di_values = di_values.dropna()
            f = sp.stats.weibull_min.fit(di_values)
            cutoff = sp.stats.weibull_min.ppf(
                self.freqai_info.get("outlier_threshold", 0.999), *f
            )

        dk.data["DI_value_mean"] = pred_df_full["DI_values"].mean()
        dk.data["DI_value_std"] = pred_df_full["DI_values"].std()
        dk.data["extra_returns_per_train"]["DI_value_param1"] = f[0]
        dk.data["extra_returns_per_train"]["DI_value_param2"] = f[1]
        dk.data["extra_returns_per_train"]["DI_value_param3"] = f[2]
        dk.data["extra_returns_per_train"]["DI_cutoff"] = cutoff

        dk.data["extra_returns_per_train"]["label_period_candles"] = (
            self.get_optuna_params(pair, "label").get("label_period_candles")
        )
        dk.data["extra_returns_per_train"]["label_natr_ratio"] = self.get_optuna_params(
            pair, "label"
        ).get("label_natr_ratio")

        dk.data["extra_returns_per_train"]["hp_rmse"] = self.get_optuna_value(
            pair, "hp"
        )
        dk.data["extra_returns_per_train"]["train_rmse"] = self.get_optuna_value(
            pair, "train"
        )

    def eval_set_and_weights(
        self, X_test: pd.DataFrame, y_test: pd.DataFrame, test_weights: np.ndarray
    ) -> tuple[list[tuple] | None, list[np.ndarray] | None]:
        if self.data_split_parameters.get("test_size", TEST_SIZE) == 0:
            eval_set = None
            eval_weights = None
        else:
            eval_set = [(X_test, y_test)]
            eval_weights = [test_weights]

        return eval_set, eval_weights

    def min_max_pred(
        self,
        pred_df: pd.DataFrame,
        fit_live_predictions_candles: int,
        label_period_candles: int,
    ) -> tuple[float, float]:
        temperature = float(
            self.freqai_info.get("prediction_thresholds_temperature", 250.0)
        )
        extrema = pred_df[EXTREMA_COLUMN].iloc[
            -(
                max(2, int(fit_live_predictions_candles / label_period_candles))
                * label_period_candles
            ) :
        ]
        min_pred = smoothed_min(extrema, temperature=temperature)
        max_pred = smoothed_max(extrema, temperature=temperature)
        return min_pred, max_pred

    def get_multi_objective_study_best_trial(
        self, namespace: str, study: optuna.study.Study
    ) -> Optional[optuna.trial.FrozenTrial]:
        if namespace != "label":
            raise ValueError(f"Invalid namespace: {namespace}")
        n_objectives = len(study.directions)
        if n_objectives < 2:
            raise ValueError(
                f"Multi-objective study must have at least 2 objectives, but got {n_objectives}"
            )
        if not QuickAdapterRegressorV3.optuna_study_has_best_trials(study):
            return None

        label_metric = self.ft_params.get("label_metric", "euclidean")
        metrics = {
            "braycurtis",
            "canberra",
            "chebyshev",
            "cityblock",
            "correlation",
            "cosine",
            "dice",
            "euclidean",
            "hamming",
            "jaccard",
            "jensenshannon",
            "kulczynski1",
            "mahalanobis",
            "matching",
            "minkowski",
            "rogerstanimoto",
            "russellrao",
            "seuclidean",
            "sokalmichener",
            "sokalsneath",
            "sqeuclidean",
            "yule",
            "hellinger",
            "geometric_mean",
            "harmonic_mean",
            "power_mean",
            "weighted_sum",
            "d1",
            "d2",
        }
        if label_metric not in metrics:
            raise ValueError(
                f"Unsupported label metric: {label_metric}. Supported metrics are {', '.join(metrics)}"
            )

        best_trials = [
            trial
            for trial in study.best_trials
            if (
                trial.values is not None
                and len(trial.values) == n_objectives
                and all(
                    isinstance(value, (int, float)) and not np.isnan(value)
                    for value in trial.values
                )
            )
        ]
        if not best_trials:
            return None

        def calculate_distances(
            normalized_matrix: np.ndarray,
            metric: str,
            p_order: float,
        ) -> np.ndarray:
            np_weights = np.array(
                self.ft_params.get("label_weights", [1.0] * normalized_matrix.shape[1])
            )
            if np_weights.size != normalized_matrix.shape[1]:
                raise ValueError("label_weights length must match number of objectives")
            ideal_point = np.ones(normalized_matrix.shape[1])

            if metric in {
                "braycurtis",
                "canberra",
                "chebyshev",
                "cityblock",
                "correlation",
                "cosine",
                "dice",
                "euclidean",
                "hamming",
                "jaccard",
                "jensenshannon",
                "kulczynski1",  # deprecated since version 1.15.0
                "mahalanobis",
                "matching",
                "minkowski",
                "rogerstanimoto",
                "russellrao",
                "seuclidean",
                "sokalmichener",  # deprecated since version 1.15.0
                "sokalsneath",
                "sqeuclidean",
                "yule",
            }:
                cdist_kwargs = {"w": np_weights}
                if metric in {
                    "jensenshannon",
                    "mahalanobis",
                    "seuclidean",
                }:
                    del cdist_kwargs["w"]
                if metric == "minkowski" and isinstance(p_order, float):
                    cdist_kwargs["p"] = p_order
                return sp.spatial.distance.cdist(
                    normalized_matrix,
                    ideal_point.reshape(1, -1),  # reshape ideal_point to 2D
                    metric=metric,
                    **cdist_kwargs,
                ).flatten()
            elif metric == "hellinger":
                return np.sqrt(
                    np.sum(
                        np_weights
                        * (np.sqrt(normalized_matrix) - np.sqrt(ideal_point)) ** 2,
                        axis=1,
                    )
                )
            elif metric in {"geometric_mean", "harmonic_mean", "power_mean"}:
                p = {
                    "geometric_mean": 0.0,
                    "harmonic_mean": -1.0,
                    "power_mean": p_order,
                }[metric]
                return sp.stats.pmean(
                    ideal_point, p=p, weights=np_weights
                ) - sp.stats.pmean(normalized_matrix, p=p, weights=np_weights, axis=1)
            elif metric == "weighted_sum":
                return np.sum(np_weights * (ideal_point - normalized_matrix), axis=1)
            elif metric == "d1":
                if normalized_matrix.shape[0] < 2:
                    return np.full(normalized_matrix.shape[0], np.inf)
                nbrs = sklearn.neighbors.NearestNeighbors(n_neighbors=2).fit(
                    normalized_matrix
                )
                distances, _ = nbrs.kneighbors(normalized_matrix)
                return distances[:, 1]
            elif metric == "d2":
                if normalized_matrix.shape[0] < 2:
                    return np.full(normalized_matrix.shape[0], np.inf)
                k = min(4, normalized_matrix.shape[0] - 1) + 1
                nbrs = sklearn.neighbors.NearestNeighbors(n_neighbors=k).fit(
                    normalized_matrix
                )
                distances, _ = nbrs.kneighbors(normalized_matrix)
                return np.mean(distances[:, 1:], axis=1)
            else:
                raise ValueError(f"Unsupported distance metric: {metric}")

        objective_values_matrix = np.array([trial.values for trial in best_trials])
        normalized_matrix = np.zeros_like(objective_values_matrix, dtype=float)

        for i in range(objective_values_matrix.shape[1]):
            current_column = objective_values_matrix[:, i]
            current_direction = study.directions[i]

            is_neg_inf_mask = np.isneginf(current_column)
            is_pos_inf_mask = np.isposinf(current_column)
            if current_direction == optuna.study.StudyDirection.MAXIMIZE:
                normalized_matrix[is_neg_inf_mask, i] = 0.0
                normalized_matrix[is_pos_inf_mask, i] = 1.0
            else:
                normalized_matrix[is_neg_inf_mask, i] = 1.0
                normalized_matrix[is_pos_inf_mask, i] = 0.0

            is_finite_mask = np.isfinite(current_column)

            if is_finite_mask.any():
                finite_col = current_column[is_finite_mask]
                finite_min_val = np.min(finite_col)
                finite_max_val = np.max(finite_col)
                finite_range_val = finite_max_val - finite_min_val

                if np.isclose(finite_range_val, 0):
                    if is_pos_inf_mask.any() and is_neg_inf_mask.any():
                        normalized_matrix[is_finite_mask, i] = 0.5
                    elif is_pos_inf_mask.any():
                        normalized_matrix[is_finite_mask, i] = (
                            0.0
                            if current_direction == optuna.study.StudyDirection.MAXIMIZE
                            else 1.0
                        )
                    elif is_neg_inf_mask.any():
                        normalized_matrix[is_finite_mask, i] = (
                            1.0
                            if current_direction == optuna.study.StudyDirection.MAXIMIZE
                            else 0.0
                        )
                    else:
                        normalized_matrix[is_finite_mask, i] = 0.5
                else:
                    if current_direction == optuna.study.StudyDirection.MAXIMIZE:
                        normalized_matrix[is_finite_mask, i] = (
                            finite_col - finite_min_val
                        ) / finite_range_val
                    else:
                        normalized_matrix[is_finite_mask, i] = (
                            finite_max_val - finite_col
                        ) / finite_range_val

        trial_distances = calculate_distances(
            normalized_matrix,
            metric=label_metric,
            p_order=float(self.ft_params.get("label_p_order", 2.0)),
        )

        return best_trials[np.argmin(trial_distances)]

    def optuna_optimize(
        self,
        pair: str,
        namespace: str,
        objective: Callable[[optuna.trial.Trial], float],
        direction: Optional[optuna.study.StudyDirection] = None,
        directions: Optional[list[optuna.study.StudyDirection]] = None,
    ) -> None:
        is_study_single_objective = direction is not None and directions is None
        if not is_study_single_objective and len(directions) < 2:
            raise ValueError(
                "Multi-objective study must have at least 2 directions specified"
            )

        study = self.optuna_create_study(
            pair=pair,
            namespace=namespace,
            direction=direction,
            directions=directions,
        )
        if not study:
            return

        if self._optuna_config.get("warm_start"):
            self.optuna_enqueue_previous_best_params(pair, namespace, study)

        if is_study_single_objective is True:
            objective_type = "single"
        else:
            objective_type = "multi"
        logger.info(
            f"Optuna {pair} {namespace} {objective_type} objective hyperopt started"
        )
        start_time = time.time()
        try:
            study.optimize(
                objective,
                n_trials=self._optuna_config.get("n_trials"),
                n_jobs=self._optuna_config.get("n_jobs"),
                timeout=self._optuna_config.get("timeout"),
                gc_after_trial=True,
            )
        except Exception as e:
            time_spent = time.time() - start_time
            logger.error(
                f"Optuna {pair} {namespace} {objective_type} objective hyperopt failed ({time_spent:.2f} secs): {str(e)}",
                exc_info=True,
            )
            return

        time_spent = time.time() - start_time
        if is_study_single_objective:
            if not QuickAdapterRegressorV3.optuna_study_has_best_trial(study):
                logger.error(
                    f"Optuna {pair} {namespace} {objective_type} objective hyperopt failed ({time_spent:.2f} secs): no study best trial found"
                )
                return
            self.set_optuna_value(pair, namespace, study.best_value)
            self.set_optuna_params(pair, namespace, study.best_params)
            study_results = {
                "value": self.get_optuna_value(pair, namespace),
                **self.get_optuna_params(pair, namespace),
            }
            metric_log_msg = ""
        else:
            best_trial = self.get_multi_objective_study_best_trial("label", study)
            if not best_trial:
                logger.error(
                    f"Optuna {pair} {namespace} {objective_type} objective hyperopt failed ({time_spent:.2f} secs): no study best trial found"
                )
                return
            self.set_optuna_values(pair, namespace, best_trial.values)
            self.set_optuna_params(pair, namespace, best_trial.params)
            study_results = {
                "values": self.get_optuna_values(pair, namespace),
                **self.get_optuna_params(pair, namespace),
            }
            metric_log_msg = (
                f" using {self.ft_params.get('label_metric', 'euclidean')} metric"
            )
        logger.info(
            f"Optuna {pair} {namespace} {objective_type} objective done{metric_log_msg} ({time_spent:.2f} secs)"
        )
        for key, value in study_results.items():
            logger.info(
                f"Optuna {pair} {namespace} {objective_type} objective hyperopt | {key:>20s} : {value}"
            )
        self.optuna_save_best_params(pair, namespace)

    def optuna_storage(self, pair: str) -> optuna.storages.BaseStorage:
        storage_dir = self.full_path
        storage_filename = f"optuna-{pair.split('/')[0]}"
        storage_backend = self._optuna_config.get("storage")
        if storage_backend == "sqlite":
            storage = optuna.storages.RDBStorage(
                url=f"sqlite:///{storage_dir}/{storage_filename}.sqlite",
                heartbeat_interval=60,
                failed_trial_callback=optuna.storages.RetryFailedTrialCallback(
                    max_retry=3
                ),
            )
        elif storage_backend == "file":
            storage = optuna.storages.JournalStorage(
                optuna.storages.journal.JournalFileBackend(
                    f"{storage_dir}/{storage_filename}.log"
                )
            )
        else:
            raise ValueError(
                f"Unsupported optuna storage backend: {storage_backend}. Supported backends are 'sqlite' and 'file'"
            )
        return storage

    def optuna_create_study(
        self,
        pair: str,
        namespace: str,
        direction: Optional[optuna.study.StudyDirection] = None,
        directions: Optional[list[optuna.study.StudyDirection]] = None,
    ) -> Optional[optuna.study.Study]:
        identifier = self.freqai_info.get("identifier")
        study_name = f"{identifier}-{pair}-{namespace}"
        try:
            storage = self.optuna_storage(pair)
        except Exception as e:
            logger.error(
                f"Failed to create optuna storage for study {study_name}: {str(e)}",
                exc_info=True,
            )
            return None

        continuous = self._optuna_config.get("continuous")
        if continuous:
            QuickAdapterRegressorV3.optuna_study_delete(study_name, storage)

        try:
            return optuna.create_study(
                study_name=study_name,
                sampler=optuna.samplers.TPESampler(
                    n_startup_trials=self._optuna_config.get("n_startup_trials"),
                    multivariate=True,
                    group=True,
                    seed=self._optuna_config.get("seed"),
                ),
                pruner=optuna.pruners.HyperbandPruner(min_resource=3),
                direction=direction,
                directions=directions,
                storage=storage,
                load_if_exists=not continuous,
            )
        except Exception as e:
            logger.error(
                f"Failed to create optuna study {study_name}: {str(e)}", exc_info=True
            )
            return None

    def optuna_enqueue_previous_best_params(
        self, pair: str, namespace: str, study: optuna.study.Study
    ) -> None:
        best_params = self.get_optuna_params(pair, namespace)
        if best_params:
            study.enqueue_trial(best_params)
        else:
            best_params = self.optuna_load_best_params(pair, namespace)
            if best_params:
                study.enqueue_trial(best_params)

    def optuna_save_best_params(self, pair: str, namespace: str) -> None:
        best_params_path = Path(
            self.full_path / f"optuna-{namespace}-best-params-{pair.split('/')[0]}.json"
        )
        try:
            with best_params_path.open("w", encoding="utf-8") as write_file:
                json.dump(self.get_optuna_params(pair, namespace), write_file, indent=4)
        except Exception as e:
            logger.error(
                f"Failed to save optuna {namespace} best params for {pair}: {str(e)}",
                exc_info=True,
            )
            raise

    def optuna_load_best_params(self, pair: str, namespace: str) -> Optional[dict]:
        best_params_path = Path(
            self.full_path / f"optuna-{namespace}-best-params-{pair.split('/')[0]}.json"
        )
        if best_params_path.is_file():
            with best_params_path.open("r", encoding="utf-8") as read_file:
                return json.load(read_file)
        return None

    @staticmethod
    def optuna_study_delete(
        study_name: str, storage: optuna.storages.BaseStorage
    ) -> None:
        try:
            optuna.delete_study(study_name=study_name, storage=storage)
        except Exception:
            pass

    @staticmethod
    def optuna_study_load(
        study_name: str, storage: optuna.storages.BaseStorage
    ) -> Optional[optuna.study.Study]:
        try:
            study = optuna.load_study(study_name=study_name, storage=storage)
        except Exception:
            study = None
        return study

    @staticmethod
    def optuna_study_has_best_trial(study: Optional[optuna.study.Study]) -> bool:
        if study is None:
            return False
        try:
            _ = study.best_trial
            return True
        # file backend storage raises KeyError
        except KeyError:
            return False
        # sqlite backend storage raises ValueError
        except ValueError:
            return False

    @staticmethod
    def optuna_study_has_best_trials(study: Optional[optuna.study.Study]) -> bool:
        if study is None:
            return False
        try:
            _ = study.best_trials
            return True
        # file backend storage raises KeyError
        except KeyError:
            return False
        # sqlite backend storage raises ValueError
        except ValueError:
            return False


def get_callbacks(trial: optuna.trial.Trial, regressor: str) -> list[Callable]:
    if regressor == "xgboost":
        callbacks = [
            optuna.integration.XGBoostPruningCallback(trial, "validation_0-rmse")
        ]
    elif regressor == "lightgbm":
        callbacks = [optuna.integration.LightGBMPruningCallback(trial, "rmse")]
    else:
        raise ValueError(f"Unsupported regressor model: {regressor}")
    return callbacks


def fit_regressor(
    regressor: str,
    X: pd.DataFrame,
    y: pd.DataFrame,
    train_weights: np.ndarray,
    eval_set: Optional[list[tuple]],
    eval_weights: Optional[list[np.ndarray]],
    model_training_parameters: dict,
    init_model: Any = None,
    callbacks: list[Callable] = None,
) -> Any:
    if regressor == "xgboost":
        from xgboost import XGBRegressor

        model = XGBRegressor(
            objective="reg:squarederror",
            eval_metric="rmse",
            callbacks=callbacks,
            **model_training_parameters,
        )
        model.fit(
            X=X,
            y=y,
            sample_weight=train_weights,
            eval_set=eval_set,
            sample_weight_eval_set=eval_weights,
            xgb_model=init_model,
        )
    elif regressor == "lightgbm":
        from lightgbm import LGBMRegressor

        model = LGBMRegressor(objective="regression", **model_training_parameters)
        model.fit(
            X=X,
            y=y,
            sample_weight=train_weights,
            eval_set=eval_set,
            eval_sample_weight=eval_weights,
            eval_metric="rmse",
            init_model=init_model,
            callbacks=callbacks,
        )
    else:
        raise ValueError(f"Unsupported regressor model: {regressor}")
    return model


def train_objective(
    trial: optuna.trial.Trial,
    regressor: str,
    X: pd.DataFrame,
    y: pd.DataFrame,
    train_weights: np.ndarray,
    X_test: pd.DataFrame,
    y_test: pd.DataFrame,
    test_weights: np.ndarray,
    test_size: float,
    fit_live_predictions_candles: int,
    candles_step: int,
    model_training_parameters: dict,
) -> float:
    min_train_window: int = fit_live_predictions_candles * int(round(1 / test_size - 1))
    max_train_window: int = len(X)
    if max_train_window < min_train_window:
        min_train_window = max_train_window
    train_window: int = trial.suggest_int(
        "train_period_candles", min_train_window, max_train_window, step=candles_step
    )
    X = X.iloc[-train_window:]
    y = y.iloc[-train_window:]
    train_weights = train_weights[-train_window:]

    min_test_window: int = fit_live_predictions_candles
    max_test_window: int = len(X_test)
    if max_test_window < min_test_window:
        min_test_window = max_test_window
    test_window: int = trial.suggest_int(
        "test_period_candles", min_test_window, max_test_window, step=candles_step
    )
    X_test = X_test.iloc[-test_window:]
    y_test = y_test.iloc[-test_window:]
    test_weights = test_weights[-test_window:]

    model = fit_regressor(
        regressor=regressor,
        X=X,
        y=y,
        train_weights=train_weights,
        eval_set=[(X_test, y_test)],
        eval_weights=[test_weights],
        model_training_parameters=model_training_parameters,
        callbacks=get_callbacks(trial, regressor),
    )
    y_pred = model.predict(X_test)

    error = sklearn.metrics.root_mean_squared_error(
        y_test, y_pred, sample_weight=test_weights
    )

    return error


def get_optuna_study_model_parameters(
    trial: optuna.trial.Trial, regressor: str
) -> dict:
    study_model_parameters = {
        "learning_rate": trial.suggest_float("learning_rate", 1e-3, 0.3, log=True),
        "min_child_weight": trial.suggest_float(
            "min_child_weight", 1e-8, 100.0, log=True
        ),
        "subsample": trial.suggest_float("subsample", 0.5, 1.0),
        "colsample_bytree": trial.suggest_float("colsample_bytree", 0.5, 1.0),
        "reg_alpha": trial.suggest_float("reg_alpha", 1e-8, 100.0, log=True),
        "reg_lambda": trial.suggest_float("reg_lambda", 1e-8, 100.0, log=True),
    }
    if regressor == "xgboost":
        study_model_parameters.update(
            {
                "max_depth": trial.suggest_int("max_depth", 3, 15),
                "gamma": trial.suggest_float("gamma", 1e-8, 10.0, log=True),
            }
        )
    elif regressor == "lightgbm":
        study_model_parameters.update(
            {
                "num_leaves": trial.suggest_int("num_leaves", 8, 256),
                "min_split_gain": trial.suggest_float(
                    "min_split_gain", 1e-8, 10.0, log=True
                ),
                "min_child_samples": trial.suggest_int("min_child_samples", 10, 100),
            }
        )
    return study_model_parameters


def hp_objective(
    trial: optuna.trial.Trial,
    regressor: str,
    X: pd.DataFrame,
    y: pd.DataFrame,
    train_weights: np.ndarray,
    X_test: pd.DataFrame,
    y_test: pd.DataFrame,
    test_weights: np.ndarray,
    model_training_parameters: dict,
) -> float:
    study_model_parameters = get_optuna_study_model_parameters(trial, regressor)
    model_training_parameters = {**model_training_parameters, **study_model_parameters}

    model = fit_regressor(
        regressor=regressor,
        X=X,
        y=y,
        train_weights=train_weights,
        eval_set=[(X_test, y_test)],
        eval_weights=[test_weights],
        model_training_parameters=model_training_parameters,
        callbacks=get_callbacks(trial, regressor),
    )
    y_pred = model.predict(X_test)

    error = sklearn.metrics.root_mean_squared_error(
        y_test, y_pred, sample_weight=test_weights
    )

    return error


def calculate_quantile(values: np.ndarray, value: float) -> float:
    if values.size == 0:
        return np.nan

    first_value = values[0]
    if np.all(np.isclose(values, first_value)):
        return (
            0.5
            if np.isclose(value, first_value)
            else (0.0 if value < first_value else 1.0)
        )

    return np.sum(values <= value) / values.size


class TrendDirection(IntEnum):
    NEUTRAL = 0
    UP = 1
    DOWN = -1


def zigzag(
    df: pd.DataFrame,
    natr_period: int = 14,
    natr_ratio: float = 6.0,
) -> tuple[list[int], list[float], list[int]]:
    min_confirmation_window: int = 2
    max_confirmation_window: int = 5
    n = len(df)
    if df.empty or n < max(natr_period, 2 * max_confirmation_window + 1):
        return [], [], []

    natr_values_cache: dict[int, np.ndarray] = {}

    def get_natr_values(period: int) -> np.ndarray:
        if period not in natr_values_cache:
            natr_values_cache[period] = (
                ta.NATR(df, timeperiod=period).fillna(method="bfill") / 100.0
            ).values
        return natr_values_cache[period]

    indices = df.index.tolist()
    thresholds = get_natr_values(natr_period) * natr_ratio
    closes = df["close"].values
    highs = df["high"].values
    lows = df["low"].values

    state: TrendDirection = TrendDirection.NEUTRAL
    depth = -1

    last_pivot_pos = -1
    pivots_indices, pivots_values, pivots_directions = [], [], []

    candidate_pivot_pos = -1
    candidate_pivot_value = np.nan
    candidate_pivot_direction: TrendDirection = TrendDirection.NEUTRAL

    volatility_quantile_cache: dict[int, float] = {}

    def calculate_volatility_quantile(pos: int) -> float:
        if pos not in volatility_quantile_cache:
            start = max(0, pos + 1 - natr_period)
            end = min(pos + 1, n)
            if start >= end:
                volatility_quantile_cache[pos] = np.nan
            else:
                natr_values = get_natr_values(natr_period)
                volatility_quantile_cache[pos] = calculate_quantile(
                    natr_values[start:end], natr_values[pos]
                )

        return volatility_quantile_cache[pos]

    def calculate_confirmation_window(
        pos: int,
        min_window: int = min_confirmation_window,
        max_window: int = max_confirmation_window,
    ) -> int:
        volatility_quantile = calculate_volatility_quantile(pos)
        if np.isnan(volatility_quantile):
            return int(round(np.median([min_window, max_window])))

        return np.clip(
            round(max_window - (max_window - min_window) * volatility_quantile),
            min_window,
            max_window,
        ).astype(int)

    def calculate_depth(
        pos: int,
        min_depth: int = 6,
        max_depth: int = 24,
    ) -> int:
        volatility_quantile = calculate_volatility_quantile(pos)
        if np.isnan(volatility_quantile):
            return int(round(np.median([min_depth, max_depth])))

        return np.clip(
            round(max_depth - (max_depth - min_depth) * volatility_quantile),
            min_depth,
            max_depth,
        ).astype(int)

    def calculate_min_slope_strength(
        pos: int,
        min_strength: float = 0.5,
        max_strength: float = 1.5,
    ) -> float:
        volatility_quantile = calculate_volatility_quantile(pos)
        if np.isnan(volatility_quantile):
            return np.median([min_strength, max_strength])

        return min_strength + (max_strength - min_strength) * volatility_quantile

    def update_candidate_pivot(pos: int, value: float, direction: TrendDirection):
        nonlocal candidate_pivot_pos, candidate_pivot_value, candidate_pivot_direction
        if 0 <= pos < n:
            candidate_pivot_pos = pos
            candidate_pivot_value = value
            candidate_pivot_direction = direction

    def reset_candidate_pivot():
        nonlocal candidate_pivot_pos, candidate_pivot_value, candidate_pivot_direction
        candidate_pivot_pos = -1
        candidate_pivot_value = np.nan
        candidate_pivot_direction = TrendDirection.NEUTRAL

    def add_pivot(pos: int, value: float, direction: TrendDirection):
        nonlocal last_pivot_pos, depth
        if pivots_indices and indices[pos] == pivots_indices[-1]:
            return
        pivots_indices.append(indices[pos])
        pivots_values.append(value)
        pivots_directions.append(direction)
        last_pivot_pos = pos
        depth = calculate_depth(pos)
        reset_candidate_pivot()

    def is_reversal_confirmed(
        candidate_pivot_pos: int,
        confirmation_start_pos: int,
        direction: TrendDirection,
        extrema_threshold: float = 0.85,
        move_away_ratio: float = 0.25,
    ) -> bool:
        confirmation_window = calculate_confirmation_window(candidate_pivot_pos)
        next_start = confirmation_start_pos + 1
        next_end = min(next_start + confirmation_window, n)
        previous_start = max(candidate_pivot_pos - confirmation_window, 0)
        previous_end = candidate_pivot_pos
        if next_start >= next_end or previous_start >= previous_end:
            return False

        next_slice = slice(next_start, next_end)
        next_closes = closes[next_slice]
        next_highs = highs[next_slice]
        next_lows = lows[next_slice]
        previous_slice = slice(previous_start, previous_end)
        previous_highs = highs[previous_slice]
        previous_lows = lows[previous_slice]

        local_extrema_ok = False
        if direction == TrendDirection.DOWN:
            valid_next = (
                np.sum(next_highs < highs[candidate_pivot_pos]) / len(next_highs)
                >= extrema_threshold
            )
            valid_previous = (
                np.sum(previous_highs < highs[candidate_pivot_pos])
                / len(previous_highs)
                >= extrema_threshold
            )
            local_extrema_ok = valid_next and valid_previous
        elif direction == TrendDirection.UP:
            valid_next = (
                np.sum(next_lows > lows[candidate_pivot_pos]) / len(next_lows)
                >= extrema_threshold
            )
            valid_previous = (
                np.sum(previous_lows > lows[candidate_pivot_pos]) / len(previous_lows)
                >= extrema_threshold
            )
            local_extrema_ok = valid_next and valid_previous
        if not local_extrema_ok:
            return False

        slope_ok = False
        if len(next_closes) >= 2:
            log_next_closes = np.log(next_closes)
            log_next_closes_std = np.std(log_next_closes)
            if np.isclose(log_next_closes_std, 0):
                next_slope_strength = 0
            else:
                log_next_closes_length = len(log_next_closes)
                weights = np.linspace(0.5, 1.5, log_next_closes_length)
                log_next_slope = np.polyfit(
                    range(log_next_closes_length), log_next_closes, 1, w=weights
                )[0]
                next_slope_strength = log_next_slope / log_next_closes_std
            min_slope_strength = calculate_min_slope_strength(candidate_pivot_pos)
            if direction == TrendDirection.DOWN:
                slope_ok = next_slope_strength < -min_slope_strength
            elif direction == TrendDirection.UP:
                slope_ok = next_slope_strength > min_slope_strength
        if not slope_ok:
            return False

        significant_move_away_ok = False
        if direction == TrendDirection.DOWN:
            if np.any(
                next_lows
                < highs[candidate_pivot_pos]
                * (1 - thresholds[candidate_pivot_pos] * move_away_ratio)
            ):
                significant_move_away_ok = True
        elif direction == TrendDirection.UP:
            if np.any(
                next_highs
                > lows[candidate_pivot_pos]
                * (1 + thresholds[candidate_pivot_pos] * move_away_ratio)
            ):
                significant_move_away_ok = True
        return significant_move_away_ok

    start_pos = 0
    initial_high_pos = start_pos
    initial_low_pos = start_pos
    initial_high = highs[initial_high_pos]
    initial_low = lows[initial_low_pos]
    for i in range(start_pos + 1, n):
        current_high = highs[i]
        current_low = lows[i]
        if current_high > initial_high:
            initial_high, initial_high_pos = current_high, i
        if current_low < initial_low:
            initial_low, initial_low_pos = current_low, i

        initial_move_from_high = (initial_high - current_low) / initial_high
        initial_move_from_low = (current_high - initial_low) / initial_low
        is_initial_high_move_significant = (
            initial_move_from_high >= thresholds[initial_high_pos]
        )
        is_initial_low_move_significant = (
            initial_move_from_low >= thresholds[initial_low_pos]
        )
        if is_initial_high_move_significant and is_initial_low_move_significant:
            if initial_move_from_high > initial_move_from_low:
                if is_reversal_confirmed(
                    initial_high_pos, initial_high_pos, TrendDirection.DOWN
                ):
                    add_pivot(initial_high_pos, initial_high, TrendDirection.UP)
                    state = TrendDirection.DOWN
                    break
            else:
                if is_reversal_confirmed(
                    initial_low_pos, initial_low_pos, TrendDirection.UP
                ):
                    add_pivot(initial_low_pos, initial_low, TrendDirection.DOWN)
                    state = TrendDirection.UP
                    break
        else:
            if is_initial_high_move_significant and is_reversal_confirmed(
                initial_high_pos, initial_high_pos, TrendDirection.DOWN
            ):
                add_pivot(initial_high_pos, initial_high, TrendDirection.UP)
                state = TrendDirection.DOWN
                break
            elif is_initial_low_move_significant and is_reversal_confirmed(
                initial_low_pos, initial_low_pos, TrendDirection.UP
            ):
                add_pivot(initial_low_pos, initial_low, TrendDirection.DOWN)
                state = TrendDirection.UP
                break
    else:
        return [], [], []

    if n - last_pivot_pos - 1 < depth:
        return pivots_indices, pivots_values, pivots_directions

    for i in range(last_pivot_pos + 1, n):
        current_high = highs[i]
        current_low = lows[i]

        if state == TrendDirection.UP:
            if np.isnan(candidate_pivot_value) or current_high > candidate_pivot_value:
                update_candidate_pivot(i, current_high, TrendDirection.UP)
            if (
                (candidate_pivot_value - current_low) / candidate_pivot_value
                >= thresholds[candidate_pivot_pos]
                and (candidate_pivot_pos - last_pivot_pos) >= depth
                and is_reversal_confirmed(candidate_pivot_pos, i, TrendDirection.DOWN)
            ):
                add_pivot(candidate_pivot_pos, candidate_pivot_value, TrendDirection.UP)
                state = TrendDirection.DOWN
        elif state == TrendDirection.DOWN:
            if np.isnan(candidate_pivot_value) or current_low < candidate_pivot_value:
                update_candidate_pivot(i, current_low, TrendDirection.DOWN)
            if (
                (current_high - candidate_pivot_value) / candidate_pivot_value
                >= thresholds[candidate_pivot_pos]
                and (candidate_pivot_pos - last_pivot_pos) >= depth
                and is_reversal_confirmed(candidate_pivot_pos, i, TrendDirection.UP)
            ):
                add_pivot(
                    candidate_pivot_pos, candidate_pivot_value, TrendDirection.DOWN
                )
                state = TrendDirection.UP

    return pivots_indices, pivots_values, pivots_directions


def label_objective(
    trial: optuna.trial.Trial,
    df: pd.DataFrame,
    fit_live_predictions_candles: int,
    candles_step: int,
) -> tuple[float, int]:
    min_label_period_candles: int = round_to_nearest_int(
        max(fit_live_predictions_candles // 16, candles_step), candles_step
    )
    max_label_period_candles: int = round_to_nearest_int(
        max(fit_live_predictions_candles // 4, min_label_period_candles),
        candles_step,
    )
    label_period_candles = trial.suggest_int(
        "label_period_candles",
        min_label_period_candles,
        max_label_period_candles,
        step=candles_step,
    )
    label_natr_ratio = trial.suggest_float("label_natr_ratio", 4.0, 16.0, step=0.01)

    df = df.iloc[
        -(
            max(2, int(fit_live_predictions_candles / label_period_candles))
            * label_period_candles
        ) :
    ]

    if df.empty:
        return -np.inf, -np.inf

    _, pivots_values, _ = zigzag(
        df,
        natr_period=label_period_candles,
        natr_ratio=label_natr_ratio,
    )

    if len(pivots_values) < 2:
        return -np.inf, -np.inf

    scaled_natr_label_period_candles = (
        ta.NATR(df, timeperiod=label_period_candles).fillna(method="bfill") / 100.0
    ) * label_natr_ratio

    return scaled_natr_label_period_candles.median(), len(pivots_values)


def smoothed_max(series: pd.Series, temperature=1.0) -> float:
    data_array = series.to_numpy()
    if data_array.size == 0:
        return np.nan
    if temperature < 0:
        raise ValueError("temperature must be non-negative")
    if np.isclose(temperature, 0):
        return data_array.max()
    return sp.special.logsumexp(temperature * data_array) / temperature


def smoothed_min(series: pd.Series, temperature=1.0) -> float:
    data_array = series.to_numpy()
    if data_array.size == 0:
        return np.nan
    if temperature < 0:
        raise ValueError("temperature must be non-negative")
    if np.isclose(temperature, 0):
        return data_array.min()
    return -sp.special.logsumexp(-temperature * data_array) / temperature


def boltzmann_operator(series: pd.Series, alpha: float) -> float:
    """
    Compute the Boltzmann operator of a series with parameter alpha.
    """
    data_array = series.to_numpy()
    if data_array.size == 0:
        return np.nan
    if alpha == 0:
        return np.mean(data_array)
    scaled_data = alpha * data_array
    shifted_exponentials = np.exp(scaled_data - np.max(scaled_data))
    numerator = np.sum(data_array * shifted_exponentials)
    denominator = np.sum(shifted_exponentials)
    return numerator / denominator


def round_to_nearest_int(value: float, step: int) -> int:
    """
    Round a value to the nearest multiple of a given step.
    :param value: The value to round.
    :param step: The step size to round to (must be non-zero).
    :return: The rounded value.
    :raises ValueError: If step is zero.
    """
    if step == 0:
        raise ValueError("step must be non-zero")
    return int(round(value / step) * step)
